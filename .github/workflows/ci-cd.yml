name: CI/CD â€“ build, dvc pull, deploy

on:
  push:
    branches:
      - ci/cd-deploy

permissions:
  contents: read
  id-token: write

env:
  IMAGE_NAME: ejahdilan/churn-model
  REMOTE_DEPLOY_DIR: /home/churnsvc/mlflow-app-deploy
  LATEST_TAG: v1
  DVC_DATA_DIR: ./data

# jobs:
# Hello team ,just mimicing a scenario, will remove the unit test from here later.

jobs:
  # test:
  #   name: Unit tests
  #   runs-on: ubuntu-latest
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v3

  #     - uses: actions/setup-python@v5
  #       name: Setup Python
  #       with:
  #         python-version: '3.13'

  #     - name: Install pip
  #       run: python -m pip install --upgrade pip

  #     - name: Check for requirements.txt
  #       run: |
  #         if [ ! -f requirements.txt ]; then
  #           echo "requirements.txt not found!"
  #           exit 1
  #         fi

  #     - name: Install dependencies
  #       run: pip install -r requirements.txt

  #     - name: Test with Pytest
  #       run: |
  #         set -a
  #         source .env.example
  #         set +a
  #         echo "Loaded environment variables:"
  #         env | grep POSTGRES_
  #         pytest




  # Depends on your existing test workflow - wait for it to succeed
  build-and-push:
    name: Build & push Docker image
    runs-on: ubuntu-latest
    # needs: test 
    outputs:
      image_full: ${{ steps.set-tags.outputs.image_full }}
      image_sha: ${{ steps.set-tags.outputs.image_sha }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up QEMU (multi-arch support)
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver: docker
          buildkitd-flags: --debug

      - name: Cache Docker layers
        uses: actions/cache@v4
        id: docker-cache
        with:
          path: /tmp/.buildx-cache
          key: buildx-cache-${{ github.sha }}
          restore-keys: |
            buildx-cache-

      - name: Set image tags
        id: set-tags
        run: |
          SHORT_SHA=${GITHUB_SHA::8}
          IMAGE_SHA_TAG=${IMAGE_NAME}:${SHORT_SHA}
          IMAGE_LATEST_TAG=${IMAGE_NAME}:${LATEST_TAG}
          echo "image_sha=${IMAGE_SHA_TAG}" >> $GITHUB_OUTPUT
          echo "image_full=${IMAGE_LATEST_TAG}" >> $GITHUB_OUTPUT

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          registry: docker.io
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push image (multi-platform ready)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: |
            ${{ steps.set-tags.outputs.image_sha }}
            ${{ steps.set-tags.outputs.image_full }}
          cache-from: type=inline
          cache-to: type=inline,mode=max
          builder: default

  # Pull latest data with DVC and deploy docker-compose on Azure VM
  deploy-to-azure-vm:
    name: Deploy to Azure VM
    runs-on: ubuntu-latest
    needs: build-and-push

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python and install DVC + Azure plugin
        uses: actions/setup-python@v4
        with:
          python-version: "3.x"

      - name: Install DVC and Azure Blob Storage dependencies
        run: |
          python -m pip install --upgrade pip
          pip install dvc[azure]

      - name: Configure DVC remote with Azure Blob Storage credentials
        env:
          AZURE_STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
          AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
        run: |

          # Confirm we are in the repo root
          pwd
          ls -la

          # Initialize DVC only if needed
          if [ ! -d .dvc ]; then
            dvc init
          fi

          # Ensure data directory exists
          mkdir -p $DVC_DATA_DIR

          # Copy the .dvc metafile to the data directory
          cp cell2celltrain.csv.dvc $DVC_DATA_DIR/

          # Setup DVC remote
          dvc remote add -d azure_remote azure://telcustomerchurndata/churndata || true
          dvc remote modify --local azure_remote account_name $AZURE_STORAGE_ACCOUNT_NAME
          dvc remote modify --local azure_remote account_key $AZURE_STORAGE_ACCOUNT_KEY

          # Pull the data (downloads cell2celltrain.csv into the data dir)
          dvc pull -r azure_remote $DVC_DATA_DIR/cell2celltrain.csv.dvc

      - name: Pull latest data with DVC from Azure Blob Storage
        run: |
          dvc pull -r azure_remote ${DVC_DATA_DIR}

      - name: Prepare known_hosts for SSH to Azure VM
        id: known-hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.AZURE_SSH_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy docker-compose and data folder to Azure VM
        uses: appleboy/scp-action@v1.0.0
        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT || 22 }}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          source: |
            ./docker-compose.yml
            ./data/**/*
          target: ${{ env.REMOTE_DEPLOY_DIR }}
          strip_components: 0

      - name: Deploy on Azure VM with docker-compose
        uses: appleboy/ssh-action@v1
        env:
          REMOTE_DIR: ${{ env.REMOTE_DEPLOY_DIR }}
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT || 22 }}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          envs: REMOTE_DIR,DOCKERHUB_USERNAME,DOCKERHUB_TOKEN
          script: |
            set -euo pipefail
            mkdir -p "$REMOTE_DIR" || true
            cd "$REMOTE_DIR"

            # Docker Hub login
            echo "$DOCKERHUB_TOKEN" | docker login --username "$DOCKERHUB_USERNAME" --password-stdin

            # Pull latest images and restart containers
            if command -v docker compose >/dev/null 2>&1; then
              docker compose pull --ignore-pull-failures
              docker compose up -d --remove-orphans
            else
              echo "docker compose not found"
              exit 1
            fi

            # Cleanup dangling images optionally
            docker image prune -f || true
