name: CI/CD â€“ build, dvc pull, deploy

on:
  push:
    branches:
      - main
      - ci/cd-deploy
  workflow_dispatch:

permissions:
  contents: read
  id-token: write

env:
  REMOTE_DEPLOY_DIR: /home/churnsvc/mlflow-app-deploy
  DVC_DATA_DIR: ./data

jobs:

  # build-and-push:
  #   name: Build & push Docker image
  #   runs-on: ubuntu-latest
  #   # needs: test 
  #   outputs:
  #     image_full: ${{ steps.set-tags.outputs.image_full }}
  #     image_sha: ${{ steps.set-tags.outputs.image_sha }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

# Pull latest data with DVC and deploy docker-compose on Azure VM
deploy-to-azure-vm:
    name: Deploy to Azure VM
    runs-on: ubuntu-latest
    needs: build-and-push

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python and install DVC + Azure plugin
        uses: actions/setup-python@v4
        with:
          python-version: "3.x"

      - name: Install DVC and Azure Blob Storage dependencies
        run: |
          python -m pip install --upgrade pip
          pip install dvc[azure]

      - name: Configure DVC remote with Azure Blob Storage credentials
        env:
          AZURE_STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
          AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
        run: |

          # Confirm we are in the repo root
          pwd
          ls -la

          # # Initialize DVC only if needed
          # if [ ! -d .dvc ]; then
          #   dvc init
          # fi

          # # Ensure data directory exists
          mkdir -p $DVC_DATA_DIR

          # Copy the .dvc metafile to the data directory
          cp dvc-track/sample.csv.dvc $DVC_DATA_DIR/

          # Setup DVC remote
          dvc remote add -d azure_remote azure://telcustomerdata/churndata || true
          dvc remote modify --local azure_remote account_name $AZURE_STORAGE_ACCOUNT_NAME
          dvc remote modify --local azure_remote account_key $AZURE_STORAGE_ACCOUNT_KEY

      - name: Pulling data and listing the contents of the data directory
        run: |
          dvc pull
          echo "Listing contents of the data directory:"
          ls -la ./data
          echo "Disk usage of data directory:"
          du -sh ./data

      - name: Install Azure CLI
        run: |
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

      - name: Upload dataset folder to Azure Blob Storage
        env:
          AZURE_STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
          AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
        run: |
          az storage blob upload-batch \
            --account-name $AZURE_STORAGE_ACCOUNT_NAME \
            --destination churndata \
            --source ./data \
            --account-key $AZURE_STORAGE_ACCOUNT_KEY \
            --overwrite

      - name: Prepare known_hosts for SSH to Azure VM
        id: known-hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.AZURE_SSH_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      - name: Copy docker-compose and data folder to Azure VM
        uses: appleboy/scp-action@v1.0.0
        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT}}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          source: "./docker-compose.yml"
          target: ${{ env.REMOTE_DEPLOY_DIR }}
          strip_components: 0

      - name: Deploy on Azure VM with docker-compose
        uses: appleboy/ssh-action@v1
        env:
          REMOTE_DIR: ${{ env.REMOTE_DEPLOY_DIR }}
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
          AZURE_STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
          AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}

        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT}}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          envs: REMOTE_DIR,DOCKERHUB_USERNAME,DOCKERHUB_TOKEN,AZURE_STORAGE_ACCOUNT_NAME, AZURE_STORAGE_ACCOUNT_KEY
          script: |
            set -euo pipefail

            mkdir -p "$REMOTE_DIR" || true

            # sudo chown -R $USER:$USER $REMOTE_DIR

            # chmod -R u+rwX $REMOTE_DIR

            # find "$REMOTE_DIR/data" -maxdepth 1 -type f -name "cell2cell*" -exec rm -f {} \;

            find "$REMOTE_DIR/data" -maxdepth 1 -type f \( -name "cell2cell*" -o -name "sample*" \) -exec rm -f {} \;

            if ! command -v az &> /dev/null; then
              curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
            fi

            mkdir -p "$REMOTE_DIR/data"

            # Download data folder from Azure Blob Storage
            az storage blob download-batch \
              --account-name $AZURE_STORAGE_ACCOUNT_NAME \
              --account-key $AZURE_STORAGE_ACCOUNT_KEY \
              --source churndata \
              --destination $REMOTE_DIR/data
            
            cd "$REMOTE_DIR"

            # Docker Hub login
            echo "$DOCKERHUB_TOKEN" | docker login --username "$DOCKERHUB_USERNAME" --password-stdin

            # Pull latest images and restart containers
            if command -v docker compose >/dev/null 2>&1; then
              docker compose pull --ignore-pull-failures
              docker compose up -d --remove-orphans
            else
              echo "docker compose not found"
              exit 1
            fi

            # Cleanup dangling images
            docker image prune -f || true