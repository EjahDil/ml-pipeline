name: CI/CD â€“ build, dvc pull, deploy

on:
  push:
    branches:
      - ci/cd-deploy

permissions:
  contents: read
  id-token: write

env:
  IMAGE_NAME: ejahdilan/churn-model
  REMOTE_DEPLOY_DIR: /home/churnsvc/mlflow-app-deploy
  LATEST_TAG: v1
  DVC_DATA_DIR: ./data

# jobs:
# Hello team ,just mimicing a scenario, will remove the unit test from here later.

jobs:
  # test:
  #   name: Unit tests
  #   runs-on: ubuntu-latest
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v3

  #     - uses: actions/setup-python@v5
  #       name: Setup Python
  #       with:
  #         python-version: '3.13'

  #     - name: Install pip
  #       run: python -m pip install --upgrade pip

  #     - name: Check for requirements.txt
  #       run: |
  #         if [ ! -f requirements.txt ]; then
  #           echo "requirements.txt not found!"
  #           exit 1
  #         fi

  #     - name: Install dependencies
  #       run: pip install -r requirements.txt

  #     - name: Test with Pytest
  #       run: |
  #         set -a
  #         source .env.example
  #         set +a
  #         echo "Loaded environment variables:"
  #         env | grep POSTGRES_
  #         pytest




  # Depends on your existing test workflow - wait for it to succeed
  build-and-push:
    name: Build & push Docker image
    runs-on: ubuntu-latest
    # needs: test 
    outputs:
      image_full: ${{ steps.set-tags.outputs.image_full }}
      image_sha: ${{ steps.set-tags.outputs.image_sha }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up QEMU (multi-arch support)
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver: docker
          buildkitd-flags: --debug

      - name: Cache Docker layers
        uses: actions/cache@v4
        id: docker-cache
        with:
          path: /tmp/.buildx-cache
          key: buildx-cache-${{ github.sha }}
          restore-keys: |
            buildx-cache-

      - name: Set image tags
        id: set-tags
        run: |
          SHORT_SHA=${GITHUB_SHA::8}
          IMAGE_SHA_TAG=${IMAGE_NAME}:${SHORT_SHA}
          IMAGE_LATEST_TAG=${IMAGE_NAME}:${LATEST_TAG}
          echo "image_sha=${IMAGE_SHA_TAG}" >> $GITHUB_OUTPUT
          echo "image_full=${IMAGE_LATEST_TAG}" >> $GITHUB_OUTPUT

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          registry: docker.io
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push image (multi-platform ready)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: |
            ${{ steps.set-tags.outputs.image_sha }}
            ${{ steps.set-tags.outputs.image_full }}
          cache-from: type=inline
          cache-to: type=inline,mode=max
          builder: default

  # Pull latest data with DVC and deploy docker-compose on Azure VM
  deploy-to-azure-vm:
    name: Deploy to Azure VM
    runs-on: ubuntu-latest
    needs: build-and-push

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python and install DVC + Azure plugin
        uses: actions/setup-python@v4
        with:
          python-version: "3.x"

      - name: Install DVC and Azure Blob Storage dependencies
        run: |
          python -m pip install --upgrade pip
          pip install dvc[azure]

      - name: Configure DVC remote with Azure Blob Storage credentials
        env:
          AZURE_STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
          AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
        run: |

          # Confirm we are in the repo root
          pwd
          ls -la

          # Initialize DVC only if needed
          if [ ! -d .dvc ]; then
            dvc init
          fi

          # # Ensure data directory exists
          mkdir -p $DVC_DATA_DIR

          # # Copy the .dvc metafile to the data directory
          cell2celltrain.csv.dvc $DVC_DATA_DIR/
          cell2celltrain_small.csv.dvc $DVC_DATA_DIR/

          # Setup DVC remote
          # dvc remote add -d azure_remote azure://telcustomerchurndata/churndata || true
          # dvc remote modify --local azure_remote account_name $AZURE_STORAGE_ACCOUNT_NAME
          # dvc remote modify --local azure_remote account_key $AZURE_STORAGE_ACCOUNT_KEY

          # Pull the data (downloads cell2celltrain.csv into the data dir)
          
          
          # dvc pull
          
          # azure_remote $DVC_DATA_DIR/cell2celltrain.csv.dvc

      - name: Listing the contents of the data directory
        run: |
          #dvc pull
          # sudo chmod -R u+rwX,go+rX ./data
          # sudo chown -R $USER:$USER ./data
          echo "Listing contents of the data directory:"
          ls -la ./data
          echo "Disk usage of data directory:"
          du -sh ./data

          # -r azure_remote ${DVC_DATA_DIR}

      - name: Install Azure CLI
        run: |
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

      - name: Upload dataset folder to Azure Blob Storage
        env:
          AZURE_STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
          AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}
        run: |
          az storage blob upload-batch \
            --account-name $AZURE_STORAGE_ACCOUNT_NAME \
            --destination telcustomerchurndata \
            --source ./data \
            --account-key $AZURE_STORAGE_ACCOUNT_KEY

      - name: Prepare known_hosts for SSH to Azure VM
        id: known-hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.AZURE_SSH_HOST }} >> ~/.ssh/known_hosts
          chmod 600 ~/.ssh/known_hosts

      # - name: Ensure target subdir exists
      #   run: |
      #     mkdir -p ~/.ssh
      #     echo "${{ secrets.AZURE_SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
      #     chmod 600 ~/.ssh/deploy_key
      #     ssh -o StrictHostKeyChecking=no -i ~/.ssh/deploy_key ${{ secrets.AZURE_SSH_USER }}@${{ secrets.AZURE_SSH_HOST }} "mkdir -p ${REMOTE_DEPLOY_DIR}/data"

      - name: Copy docker-compose and data folder to Azure VM
        uses: appleboy/scp-action@v1.0.0
        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT}}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          source: "./docker-compose.yml"
          target: ${{ env.REMOTE_DEPLOY_DIR }}
          strip_components: 0

      - name: Deploy on Azure VM with docker-compose
        uses: appleboy/ssh-action@v1
        env:
          REMOTE_DIR: ${{ env.REMOTE_DEPLOY_DIR }}
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
          AZURE_STORAGE_ACCOUNT_NAME: ${{ secrets.AZURE_STORAGE_ACCOUNT_NAME }}
          AZURE_STORAGE_ACCOUNT_KEY: ${{ secrets.AZURE_STORAGE_ACCOUNT_KEY }}

        with:
          host: ${{ secrets.AZURE_SSH_HOST }}
          username: ${{ secrets.AZURE_SSH_USER }}
          port: ${{ secrets.AZURE_SSH_PORT}}
          key: ${{ secrets.AZURE_SSH_PRIVATE_KEY }}
          envs: REMOTE_DIR,DOCKERHUB_USERNAME,DOCKERHUB_TOKEN,AZURE_STORAGE_ACCOUNT_NAME, AZURE_STORAGE_ACCOUNT_KEY
          script: |
            set -euo pipefail
            
            mkdir -p "$REMOTE_DIR" || true

            if ! command -v az &> /dev/null; then
              curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
            fi

            # Download data folder from Azure Blob Storage
            az storage blob download-batch \
              --account-name $AZURE_STORAGE_ACCOUNT_NAME \
              --account-key $AZURE_STORAGE_ACCOUNT_KEY \
              --source telcustomerchurndata \
              --destination $REMOTE_DIR
            
            cd "$REMOTE_DIR"

            # Ensure Python & DVC installed on VM
            if ! command -v dvc &> /dev/null; then
                python3 -m venv .venv
                source .venv/bin/activate
                pip install --upgrade pip
                pip install "dvc[azure]"
            else
                source .venv/bin/activate
            fi

            # Initialize DVC only if needed
            if [ ! -d .dvc ]; then
              dvc init --no-scm
            fi

            # Setup Azure remote (use environment variables or secrets on the VM)
            if ! dvc remote list | grep -q "azure_remote"; then
              dvc remote add -d azure_remote azure://telcustomerchurndata/churndata
            else
              dvc remote default azure_remote
            fi

            dvc remote modify --local azure_remote account_name $AZURE_STORAGE_ACCOUNT_NAME
            dvc remote modify --local azure_remote account_key $AZURE_STORAGE_ACCOUNT_KEY

            # Pull data (will download into ./data or path defined in your dvc files)

            dvc pull

            # Docker Hub login
            echo "$DOCKERHUB_TOKEN" | docker login --username "$DOCKERHUB_USERNAME" --password-stdin

            # Pull latest images and restart containers
            if command -v docker compose >/dev/null 2>&1; then
              docker compose pull --ignore-pull-failures
              docker compose up -d --remove-orphans
            else
              echo "docker compose not found"
              exit 1
            fi

            # Cleanup dangling images optionally
            docker image prune -f || true
